{
 "cells": [
  {
   "cell_type": "code",
   "id": "54e0aa296fcad41b",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Define paths\n",
    "base_dir = '../data/DAWN_processed'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Define classes (weather conditions)\n",
    "classes = ['Fog', 'Rain', 'Snow', 'Sand']\n",
    "\n",
    "# Define parameters\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "epochs = 20  # Increased epochs for better training\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-4  # L2 regularization\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "fdbd1d567ca3e680"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.25), ratio=(0.3, 3.3))  # Added RandomErasing for robust training.\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_test_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=val_test_transforms)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=False)\n",
    "\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "print(f\"Class to idx mapping: {train_dataset.class_to_idx}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n"
   ],
   "id": "b428eba34291bd49"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define CNN model (using pre-trained ResNet18 with modifications)\n",
    "def create_pretrained_model(num_classes=4):\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    # Freeze early layers\n",
    "    for param in list(model.parameters())[:-4]:\n",
    "        param.requires_grad = False\n",
    "    # Add dropout and batch normalization after ResNet's feature extractor\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(model.fc.in_features, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_pretrained_model(num_classes=len(classes)).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # Added weight decay.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3,\n",
    "                                                 verbose=True)  # increased patience\n",
    "\n"
   ],
   "id": "6c87c62575d532d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training function (same as before)\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.train()\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        model.train()\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs} [Train]\")\n",
    "        for inputs, labels in train_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_bar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n"
   ],
   "id": "218bdcb13ba01c80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation function (same as before)\n",
    "def evaluate_model(model, data_loader, criterion=None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if criterion:\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(data_loader.dataset) if criterion else 0\n",
    "    acc = 100 * correct / total\n",
    "\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "# Plot function\n",
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax1.plot(history['train_acc'])\n",
    "    ax1.plot(history['val_acc'])\n",
    "    ax1.set_title('Model accuracy')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(history['train_loss'])\n",
    "    ax2.plot(history['val_loss'])\n",
    "    ax2.set_title('Model loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Confusion matrix function\n",
    "def plot_confusion_matrix(model, data_loader, classes):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"Generating predictions\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes,\n",
    "                yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=classes))\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n"
   ],
   "id": "d135773f45fe44ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize predictions function\n",
    "def visualize_predictions(model, data_loader, classes, num_images=12):\n",
    "    model.eval()\n",
    "\n",
    "    # Get a batch of images\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    images, labels = images[:num_images], labels[:num_images]\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted = predicted.cpu()\n",
    "\n",
    "    # Plot images with predictions\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i in range(num_images):\n",
    "        # Add subplot\n",
    "        ax = fig.add_subplot(3, 4, i + 1)\n",
    "\n",
    "        # Un-normalize the image for display\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "        # Display the image\n",
    "        ax.imshow(img)\n",
    "\n",
    "        # Add titles\n",
    "        true_label = classes[labels[i]]\n",
    "        pred_label = classes[predicted[i]]\n",
    "        title_color = 'green' if labels[i] == predicted[i] else 'red'\n",
    "        ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color=title_color)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_examples.png')\n",
    "    plt.show()\n",
    "\n"
   ],
   "id": "263de67aca2802c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to predict a single image\n",
    "def predict_image(model, image_path, classes, transform=None):\n",
    "    if transform is None:\n",
    "        transform = val_test_transforms\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    from PIL import Image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)[0]\n",
    "        _, predicted_idx = torch.max(probabilities, 0)\n",
    "\n",
    "    # Get the predicted class and confidence\n",
    "    predicted_class = classes[predicted_idx.item()]\n",
    "    confidence = probabilities[predicted_idx].item()\n",
    "\n",
    "    # Get all class probabilities\n",
    "    all_probs = {classes[i]: prob.item() for i, prob in enumerate(probabilities)}\n",
    "\n",
    "    return {\n",
    "        'class': predicted_class,\n",
    "        'confidence': confidence,\n",
    "        'all_probabilities': all_probs\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to load the saved model for later use\n",
    "def load_model(model_path, num_classes=4):\n",
    "    checkpoint = torch.load(model_path)\n",
    "\n",
    "    # Create a new model instance\n",
    "    model = create_pretrained_model(num_classes=num_classes)\n",
    "\n",
    "    # Load the saved state dict\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, checkpoint['classes']\n",
    "\n"
   ],
   "id": "dd8dfecf6e137e9d"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define multiple optimizers for experimentation\n",
    "    optimizers = {\n",
    "        \"Adam\": lambda params: optim.Adam(params, lr=learning_rate, weight_decay=weight_decay),\n",
    "        \"SGD\": lambda params: optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=weight_decay),\n",
    "        \"RMSprop\": lambda params: optim.RMSprop(params, lr=learning_rate, alpha=0.9, weight_decay=weight_decay),\n",
    "        \"Adagrad\": lambda params: optim.Adagrad(params, lr=learning_rate, weight_decay=weight_decay),\n",
    "        \"AdamW\": lambda params: optim.AdamW(params, lr=learning_rate, weight_decay=weight_decay),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for opt_name, opt_func in optimizers.items():\n",
    "        print(f\"\\nTraining with {opt_name} optimizer...\\n\")\n",
    "\n",
    "        # Create a fresh model instance\n",
    "        model = create_pretrained_model(num_classes=len(classes)).to(device)\n",
    "\n",
    "        # Define optimizer and scheduler\n",
    "        optimizer = opt_func(model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "        # Train model\n",
    "        trained_model, history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs)\n",
    "\n",
    "        # Evaluate model on test data\n",
    "        test_loss, test_acc = evaluate_model(trained_model, test_loader, criterion)\n",
    "\n",
    "        # Generate confusion matrix and classification report\n",
    "        y_true, y_pred = plot_confusion_matrix(trained_model, test_loader, classes)\n",
    "        report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
    "\n",
    "        # Store results\n",
    "        results[opt_name] = {\n",
    "            \"train_history\": history,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "\n",
    "        print(f\"Completed training with {opt_name}. Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "    # Print summary of all optimizer results\n",
    "    for opt_name, res in results.items():\n",
    "        print(f\"\\nOptimizer: {opt_name}\")\n",
    "        print(f\"Test Loss: {res['test_loss']:.4f}, Test Accuracy: {res['test_accuracy']:.2f}%\")\n",
    "        print(f\"Precision, Recall, and F1-score:\\n{res['classification_report']}\\n\")"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
